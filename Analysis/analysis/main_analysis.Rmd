---
title: "papers_foundD_100"
output: html_document
date: "2025-10-28"
---
```{r}
library(here)

source(here("Analysis/R/00_packages.R"))
source(here("Analysis/R/01_load_data.R"))
source(here("Analysis/R/02_prepare_data.R"))
source(here("Analysis/R/03_descriptives.R"))
# source(here("R/04_models.R"))
# source(here("R/05_plots.R"))

load_packages()

```

## Loading and transforming data

```{r load and transform data}

# import simulation results and metadata (specify paths relative to project root)
raw <- load_simulation_data(data = "simulation_results/inclusion_only/all_simulation_results.csv", 
                             metadata = "Analysis/percentage_relevant.csv")

# (1) transform simulation data to wide
# (2) add the number of records, percentage of relevant records and dataset topic(s) from metadata to simulation data (these will serve as group-level predictors in multilevel model)
# (4) rescale n_abstracts (divide by 100) and length_abstracts (divide by 1000)
# (5) truncate topic names to first topic only
processed <- prepare_data(raw$simulation, raw$meta)

# assign to variables for easier access
simulation_long <- raw$simulation
simulation      <- processed$simulation
metadata        <- processed$meta

colnames(simulation)[colnames(simulation) == 'papers_found'] <- 'papers_found'

```


## Descriptives

### Summary statistics

```{r}
head(simulation)
summary(simulation)

#export summary data to latex format text file ???
```





### Centering predictors

Let's start by centering the llm-specific predictors within the llm condition, and setting them to 0 in the other conditions (two-part coding). This way, the coefficients for the condition factor will represent the difference in papers_found between the llm condition (when llm parameters are at their mean) and the other conditions.

```{r}

simulation <- simulation %>%
  mutate(
    
    # indicator where llm parameters are actually defined
    llm_active = (condition == "llm"),

    # set to 0 if not llm condition (two-part coding)
    n_abstracts_llm = ifelse(
      llm_active,
      n_abstracts - mean(n_abstracts[llm_active], na.rm = TRUE),
      0
    ),
    length_abstracts_llm = ifelse(
      llm_active,
      length_abstracts  - mean(length_abstracts[llm_active], na.rm = TRUE),
      0
    ),
    llm_temperature_llm = ifelse(
      llm_active,
      llm_temperature - mean(llm_temperature[llm_active], na.rm = TRUE),
      0
    )
  )

```


### Descriptive plots

#### Bar chart of number of relevant records found per dataset and condition

```{r}

descriptive_barchart(simulation, metadata, recall_at_100)

```


## Data Analysis

### Checking assumptions


```{r}
#check the assumption of homogeneity of variance across conditions
ggplot(simulation, aes(x = condition, y = papers_found)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = .3)

#calculate Levene's test for homogeneity of variance
car::leveneTest(papers_found ~ condition, data = simulation) # not significant, so homogeneity of variance assumption met

```
There seems to be little variance in the 'criteria' condition within datasets. This makes sense since the other three conditions have additional sources of randomness (llm: LLM outputs, no_initialisation and random: random sampling). The 'criteria' condition always starts with the same two relevant records (the criteria), which likely leads to more similar performance across runs. So, even though Levene's test indicates that the homogeneity of variance assumption is met, the visualization suggests that there may be some heterogeneity of variance specifically between the criteria condition and the other conditions. Thus this comparison should be interpreted with caution.

Variance across runs in the other conditions is due to variation in priors, not to model stochasticity.

```{r, run=FALSE}}
# check the assumption of linearity between percentage and relevant records found
ggplot(simulation,
       aes(x = percent_rel, y = papers_found)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              aes(color = "quadratic"),
              se = FALSE) +
  labs(
    title = "Percentage of relevant records vs. number of relevant records found",
    x = "percentage of relevant records",
    y = "Number of relevant records found in first 100 screened"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background  = element_rect(fill = "white", color = NA),
    legend.background = element_rect(fill = "white", color = NA),
    legend.key = element_rect(fill = "white", color = NA),
    plot.margin = margin(10, 20, 10, 10)
  )

# check the assumption of linearity between percentage and relevant records found
ggplot(simulation,
       aes(x = records, y = papers_found)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              aes(color = "quadratic"),
              se = FALSE) +
  labs(
    title = "Total number of records vs. number of relevant records found",
    x = "Percentage of relevant records",
    y = "Number of relevant records found in first 100 screened"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background  = element_rect(fill = "white", color = NA),
    legend.background = element_rect(fill = "white", color = NA),
    legend.key = element_rect(fill = "white", color = NA),
    plot.margin = margin(10, 20, 10, 10)
  )

```



# Mixed effects model

A bottom-up modeling approach is used to build the mixed effects model. We start with an intercept-only model and subsequently add fixed effects and random slopes. At each step, we evaluate whether the addition of parameters significantly improves model fit using likelihood ratio tests

###

```{r}
#set reference category for regression model
simulation <- simulation %>% mutate(condition = relevel(factor(condition), ref = "llm"))
```



### Intercept only model

```{r}
model0 <- lmer(papers_found ~ 1 + (1|dataset), REML = F, data = simulation)
summary(model0)
```

```{r}
Intercept_variances = as.data.frame(VarCorr(model0))

ICC = Intercept_variances[Intercept_variances$grp == 'dataset','vcov'] / (Intercept_variances[Intercept_variances$grp == 'dataset','vcov'] + Intercept_variances[Intercept_variances$grp == 'Residual','vcov'])

print(ICC)
```

<!-- #### Grand mean centering -->

<!-- Let's see how grand mean centering affects the results. Grand mean centering involves subtracting the overall mean of a predictor from each individual value of that predictor. This transformation can help in interpreting the intercept and can also reduce multicollinearity among predictors. -->

<!-- ```{r} -->
<!-- #grand mean center the dataset level predictors -->
<!-- simulation_centered <- simulation %>% -->
<!--   mutate( -->
<!--     percent_rel_centered = percent_rel - mean(percent_rel, na.rm = TRUE), -->
<!--     records_centered = records - mean(records, na.rm = TRUE) -->
<!--   ) -->
<!-- ``` -->


<!-- Now, we'll run two models - one grand mean centered and the other not - to see if centering makes a difference in the results. -->

<!-- ```{r} -->

<!-- model_centered <- lmer(papers_found ~ percent_rel_centered + records_centered + (1|dataset), REML = F, data = simulation_centered) -->
<!-- summary(model_centered) -->

<!-- model_not_centered <- lmer(papers_found ~ percent_rel + records + (1|dataset), REML = F, data = simulation) -->
<!-- summary(model_not_centered) -->
<!-- ``` -->

<!-- The random variance components of the two models  -->


```{r}

model1 <- lmer(papers_found ~ condition  + (1|dataset), REML = F, data = simulation)
summary(model1)
anova(model1, model0)
```




```{r}

model2 <- lmer(papers_found ~ condition + n_abstracts_llm + length_abstracts_llm + llm_temperature_llm + (1|dataset), REML = F, data = simulation)
summary(model2)
anova(model2, model1)
```


15742    

```{r}
model3 <- lmer(papers_found ~ condition + percent_rel + (1|dataset), REML = F, data = simulation)
summary(model3)
anova(model3, model1)
```

```{r}
model4 <- lmer(papers_found ~ condition + percent_rel + (condition|dataset), REML = F, data = simulation)
summary(model4)
anova(model4, model3)
```

The negative correlations between the random intercepts and random slopes indicate that in datasets where performance under the reference condition (LLM) is higher than average, the drop in performance when using minimal or no_priors instead of LLM is larger than average. In other words, the effect of initialization was more pronounced for datasets where the overall performance was higher in general. Some datasets are simply easier for the active learning model of ASReview: many relevant records can be found early. In those ‘easier’ datasets, LLM-based initialization tends to create an especially large advantage over minimal or no_priors, which is reflected in the strong negative correlations between the random intercept and the random slopes.

```{r}
model5 <- lmer(papers_found ~ condition + percent_rel + condition*records + (condition|dataset), REML = F, data = simulation)
summary(model5)
anova(model5, model4)
```

### Checking normality of residuals

```{r}
#plot QQ plots of residuals
qqnorm(residuals(model5))

#plot QQ plots of random means
qqnorm(ranef(model5)$dataset[,1])

#plot QQ plots of random effect with no initialization condition 
qqnorm(ranef(model5)$dataset[,2])

#plot QQ plots of random effect with random initialization condition
qqnorm(ranef(model5)$dataset[,3])
```

The residuals and random effects do not seem to be normally distributed. 


### Is number of relevant records found a limited measure of starting performance?

```{r}
# calculate recall at 100 for each run
simulation <- simulation %>%
  mutate(
    recall_at_100 = papers_found / included * 100
  )

#the percentage of runs which have perfect recall
percent_perfect_recall <- simulation %>%
  group_by(condition) %>%
  summarise(
    percent_perfect_recall = mean(recall_at_100 == 100) * 100
  )
percent_perfect_recall

#calculate variation in recall at 100 per condition
variation_recall <- simulation %>%
  group_by(condition) %>%
  summarise(
    sd_recall_at_100 = sd(recall_at_100)
  )
variation_recall

#join the two tables
recall_summary <- percent_perfect_recall %>%
  left_join(variation_recall, by = "condition")

recall_summary
```



# Exporting data and results

# Descriptives

```{r}
#export descriptive statistics table to latex format text file
writeLines(capture.output(descriptive_tables(simulation)), here::here("Report/results/descriptives_table.tex"))

descriptive_tables(simulation)
```

## Perfect recall percentages per condition in text

```{r}
perfect_recall_text <- paste0(
  "all relevant records were found within the first 100 records screened, in ", round(percent_perfect_recall$percent_perfect_recall[percent_perfect_recall$condition == "llm"],2),
  "\\% of the runs in the LLM-initialization condition. ",
  "In the random-initialization condition, this was ", round(percent_perfect_recall$percent_perfect_recall[percent_perfect_recall$condition == "random"],2),
  "\\%, in the criteria condition, ", round(percent_perfect_recall$percent_perfect_recall[percent_perfect_recall$condition == "criteria"],2),
  "\\%, and in the no-initialization condition, ", round(percent_perfect_recall$percent_perfect_recall[percent_perfect_recall$condition == "no_initialisation"],2),
  "\\%."
)

# ---- Print LaTeX to console (or write to a .tex file)
cat(perfect_recall_text)
writeLines(perfect_recall_text, here::here("Report/results/perfect_recall_text.tex"))

```


## Model results

```{r}

p_apa <- function(p) ifelse(is.na(p), "NA", ifelse(p < .001, "$p$ < .001", sprintf("$p$ = %.3f", p)))


#undo scientific notation
options(scipen = 999)

# limit to 2 decimal places
  
# ---- Build LaTeX paragraph
latex_text <- paste0(
  "We fit a linear regression to examine whether initialization affected the number of ",
  "relevant records found within the first 100 screenings. ",
  "The LLM-initialized model yielded on average \\textit{B} = ", round(summary_basic$coefficients[1, 1],2),
  ", \\textit{SE} = ", round(summary_basic$coefficients[1, 2],2),
  ", \\textit{t} = ", round(summary_basic$coefficients[1, 3],2),
  ". ",

  "Interestingly, this is approximately equal to number of papers found in the random-initialization condition, at a difference of:, ",
  "\\textit{B} = ", round(summary_basic$coefficients[2, 1],2),
  ", \\textit{SE} = ", round(summary_basic$coefficients[2, 2],2),
  ", \\textit{t} = ", round(summary_basic$coefficients[2, 3],2),
  ", ", p_apa(summary_basic$coefficients[2, 4]),
  ". ",
  
  "In contrast, the difference between the LLM-initialization and the no-initialization conditions was statistically significant: ",
  "\\textit{B} = ", round(summary_basic$coefficients[3, 1],2),
  ", \\textit{SE} = ", round(summary_basic$coefficients[3, 2],2),
  ", \\textit{t} = ", round(summary_basic$coefficients[3, 3],2),
  ", ", p_apa(summary_basic$coefficients[3, 4]),
  ".\n",
  "Overall model fit was $R^2$ = ", round(summary_basic$r.squared,2),
  ", adjusted $R^2$ = ", round(summary_basic$adj.r.squared,2),
  ", and $F(", Df1, ", ", Df2, ")$ = ", round(summary_basic$fstatistic[1],2),
  ", \\textit{p} = ", p_apa(anova_basic$'Pr(>F)'[1]), "."
)

# ---- Print LaTeX to console (or write to a .tex file)
cat(latex_text)
writeLines(latex_text, "../Report/results/results_paragraph.tex")

```


```{r}
subset(as.data.frame(VarCorr(model2)), grp == "dataset")$vcov
subset(as.data.frame(VarCorr(model2)), grp == "Residual")$vcov
```

```{r}

random_effects <- function(model){
  
  round_to = 3
  
  if (nrow(as.data.frame(VarCorr(model))) == 2) {
    
    return(
      list(
        residual_var = round(subset(as.data.frame(VarCorr(model)), grp == "Residual")$vcov, round_to),
        intercept_var= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$vcov, round_to)
        
      )
    )
  }
  else {
    list(
      residual_var = round(subset(as.data.frame(VarCorr(model)), grp == "Residual")$vcov, round_to),
      intercept_var= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$vcov[1], round_to),
      random_init_var= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$vcov[2], round_to),
      no_init_var= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$vcov[3], round_to),
      cor_random_init_intercept= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$sdcor[4], round_to),
      cor_no_init_intercept= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$sdcor[5], round_to),
      cor_no_init_random_init= round(subset(as.data.frame(VarCorr(model)), grp == "dataset")$sdcor[6], round_to)
    )
  }
}

random_effects(model4)

```


```{r}

p_to_stars <- function(p) {
  if (is.na(p)) return("")
  if (p < 0.001) return("***")
  if (p < 0.01)  return("**")
  if (p < 0.05)  return("*")
  ""
}


stargazer(
  model1, model2, model3, model4, model5,
  # column.labels = c("Conds", "Within", "Between", "Rand effects", "Interactions"),
  # column.separate = c(1, 1, 1, 1, 1),
  type = "latex",
  title = "Effect of initialisation on the number relevant records found in first 100 screened",
  dep.var.labels = "Outcome: number of records found",
  covariate.labels = c(
     "\\multirow{2}{*}{\\shortstack[l]{Random\\\\initialisation}}",
     "\\multirow{2}{*}{\\shortstack[l]{No\\\\initialisation}}",
    "\\multirow{2}{*}{\\shortstack[l]{Number of \\\\ abstracts}}",
    "\\multirow{2}{*}{\\shortstack[l]{Length \\\\ abstracts}}",
    "\\multirow{2}{*}{\\shortstack[l]{LLM \\\\ temperature.}}",
    "\\multirow{2}{*}{\\shortstack[l]{Dataset size \\\\ (N records)}}",
    "\\multirow{2}{*}{\\shortstack[l]{\\% relevant records}}",
    "\\multirow{2}{*}{\\shortstack[l]{\\% x Random \\\\ initialisation}}",
    "\\multirow{2}{*}{\\shortstack[l]{\\% x No initialisation}}"
  ),
  add.lines = list(
    
    c("$\\mathrm{Var}(u_{0j})$",
      random_effects(model1)$intercept_var, 
      random_effects(model2)$intercept_var, 
      random_effects(model3)$intercept_var, 
      random_effects(model4)$intercept_var, 
      random_effects(model5)$intercept_var
      ),
    
    c("$\\mathrm{Var}(u_{1j})$",
      "", 
      "", 
      "", 
      random_effects(model4)$random_init_var, 
      random_effects(model5)$random_init_var
      ),
    
    c("$\\mathrm{Var}(u_{2j})$",
      "",
      "",
      "",
      random_effects(model4)$no_init_var,
      random_effects(model5)$no_init_var
      ),
    
        c("$\\mathrm{Var}(\\varepsilon_{ij})$",
      random_effects(model1)$residual_var, 
      random_effects(model2)$residual_var, 
      random_effects(model3)$residual_var, 
      random_effects(model4)$residual_var, 
      random_effects(model5)$residual_var
      ),
    
    c("$\\rho(u_{0j}, u_{1j})$",
      "",
      "",
      "",
      random_effects(model4)$cor_random_init_intercept,
      random_effects(model5)$cor_random_init_intercept
      ),
    c("$\\rho(u_{0j}, u_{2j})$",
      "",
      "",
      "",
      random_effects(model4)$cor_no_init_intercept,
      random_effects(model5)$cor_no_init_intercept
      ),
    c("$\\rho(u_{1j}, u_{2j})$",
      "",
      "",
      "",
      random_effects(model4)$cor_no_init_random_init,
      random_effects(model5)$cor_no_init_random_init
      ),
    c("$\\Delta$ deviance (df)",
      "",
      paste0(round(anova(model1, model2)$Chisq[2],2), " (", anova(model1, model2)$Df[2], ")", p_to_stars(anova(model1, model2)$'Pr(>Chisq)'[2])),
      paste0(round(anova(model2, model3)$Chisq[2],2), " (", anova(model2, model3)$Df[2], ")", p_to_stars(anova(model2, model3)$'Pr(>Chisq)'[2])),
      paste0(round(anova(model3, model4)$Chisq[2],2), " (", anova(model3, model4)$Df[2], ")", p_to_stars(anova(model3, model4)$'Pr(>Chisq)'[2])),
      paste0(round(anova(model4, model5)$Chisq[2],2), " (", anova(model4, model5)$Df[2], ")", p_to_stars(anova(model4, model5)$'Pr(>Chisq)'[2]))
      )
  ),
  #no.space = TRUE,
  keep.stat = c("aic", "ll"),
  star.cutoffs = NA,
  notes.align	= "l",
  # notes = "\\parbox{0.9\\textwidth}{Model 1 includes only initialization conditions, Model 2 adds within-dataset covariates, Model 3 adds between-dataset covariates, Model 4 adds random slopes for initialization, and Model 5 adds interaction terms. $^{*} p<0.1; ^{**} p<0.05; ^{***} p<0.01$}",
  notes.label  = "",
  notes = c(
    "Model 1: only initialization conditions.",
    "Model 2: adds within-dataset covariates.",
    "Model 3: adds between-dataset covariates.",
    "Model 4: adds random effects for initialization.",
    "Model 5: adds interaction terms.",
    "$^{*} p<0.1; ^{**} p<0.05; ^{***} p<0.01$"
  ),
  notes.append  = FALSE,
  header = TRUE,
  out = here::here("Report/results/results_table.tex")
)



```









