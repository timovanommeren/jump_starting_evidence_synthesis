% main.tex
\documentclass[12pt]{article}



% Import formatting & packages
\input{preamble}


\begin{document}

\include{titlepage}

\section{Introduction}

\subsection{Background}

\subsubsection{Active Learning Models for Systematic Reviewing}

Researchers and practitioners are continually challenged to base their decisions on the latest scientific evidence. Systematic reviews and meta-analyses were developed to address this need as rigorous methods of summarizing scientific literature \parencite{chalmers2002brief, bastian2010seventy}. However, systematically reviewing the literature can be time-consuming, which limits its practical applicability, especially, in, for example, times of crisis \parencite{tricco2020rapid, nussbaumer2021resource}.

Fortunately, recent advances in machine learning have produced tools that allow for the systematic screening of scientific literature while greatly reducing the need for manual screening \parencite{van2021open}. Specifically, active learning models (ALMs) ask users to screen titles and abstracts of papers one by one. Based on the user's decision, the models reassess the probability that the remaining papers are relevant and thus whether to show them to the user. In other words, these models continually reshuffle the papers retrieved from a scientific literature search based on the user's interests. This method reduces the time needed to find as many relevant papers as possible compared to simple index-based screening \parencite{van2025hunt}. 

\subsubsection{The Cold Start Problem}

A key challenge to using active learning for systematic reviews is that these models face a "cold start" \parencite{panda2022approaches}. For an ALM to query a user with a potentially interesting paper, the model must first have knowledge of the user's interests . One way of overcoming a cold start is to initialize, or 'warm up', the ALM using examples of relevant and irrelevant papers \parencite{teijema2025large}. If however no examples are available the user may simply start screening 'from the top-down', until a relevant and an irrelevant paper have been found. 

\subsubsection{The Advent of Large Language Models}

With the recent advent of large language models (LLMs), a new possible solution to overcoming the cold start problem has emerged \parencite{bachmann2025adaptive}. It may be advantageous to initialize the ALM with synthetic examples of relevant and irrelevant papers, rather than screening from the top-down until actual examples are found. This may be particularly true if the prevalence of relevant papers is rather low. In this case, active learning assisted screening using even a sub-optimal example of a relevant paper may be preferable to random screening for hundreds or thousands of papers. That said, the use of synthetic data may also misdirect the ALM by contaminating the model's training data. 

It may therefore be possible to overcome the cold start problem and improve starting performance by using LLMs to generate examples of relevant and irrelevant papers to initialize the ALMs for systematic reviews. This approach could be particularly useful if a user has no relevant examples available, as it avoids top-down screening. However, using synthetic data generated by LLMs seems less likely to improve starting performance if a user has access to actual examples. 


\subsection{Objectives}

This study aims to investigate the effect of using LLM-generated data to initialise active learning models (ALMs) for systematic reviews, compared to random or no initialisation, on starting performance. This will be achieved by simulating the screening of previously published systematic reviews.

% The aim is to investigate the effect \textit{on starting performance} of initializing  ALMs for systematic reviews using:
% \begin{enumerate}[label=\arabic*), leftmargin=*, nosep]
%     \item examples of relevant and irrelevant papers generated by LLMs;
%     \item examples of relevant and irrelevant papers randomly sampled from the sets of screened papers;
%     \item without any examples,
% \end{enumerate}

\section{Methodology}

\subsection{Simulation set-up}

In order to investigate the effect of LLM initialisation, random initialisation and no initialisation on starting performance, the SYNERGY datasets will be screened based on the abstracts and titles of the papers using ASReview's ALM. ASReview is an open-source software package for semi-automated systematic reviewing that implements several ALMs. To simulate the screening process, we will access ASReview via its Python API. For all simulation runs, we will use the recommended U4 configuration in ASReview, which combines a support vector machine classifier with a bi-gram term-frequency inverse document frequency (TF-IDF) feature extractor. The SYNERGY datasets consist of 26 previously screened and labelled sets of papers. Importantly, this gives us access to the ground truth labels of all the papers in these datasets. See Appendix \ref{tab:SYNERGY} for a list of all datasets including their topic, total number of records, number of records included and the percentage of relevant records \parencite{de2023synergy}. 

\subsubsection{How many runs are necessary?}

NOTE: Though power estimations don't seem to be standard practice in simulation studies, and a quick reading of the applicable literature (next paragraph) didn't give any indication that many runs would be necessary to estimate starting performance with sufficient precision, it does show metholdogical rigour to do some kind of estimation of Monte Carlo error \parencite{morris2019using, burton2006design}. The two options in the literature seem to be either to do a pilot study, or to use an analytical approach. Doing something like this in a systematic manner would be ideal. 

Previous simulation studies have found that changing the model parameters in ASReview can significantly effect starting performance, while giving different examples of relevant and irrelevant papers to initialize ASReview has minimal effect on starting performance \parencite{byrne2024impact, teijema2025large}. Thus we may conclude that by keeping ASReview's model set to the recommended default U4 configuration over all runs, the effect on starting performance due to random fluctuations should be minimal \parencite{asreviewvs2jonathan}. 

All simulations were done in \textit{Python version 3.10}. For a full list of the packages and their versions, please see  \href{https://github.com/timovanommeren/thesis_timo}{the github repository}

\vspace{0.5cm}

I am currently aiming for 50 simulation runs per condition per dataset, resulting in a total of 26 (datasets) x 3 (conditions) x 50 (runs) = 3900 simulation runs. (This calculation does not yet consider the variations due to the independent variables in the LLM initialisation condition, so the actual number of runs may be higher).


\hspace{1cm}

\begin{figure}[htbp]
  \centering
    \begin{tikzpicture}[node distance=2.5cm]
    
    \tikzstyle{data} = [rectangle, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!40]
    \tikzstyle{prompt} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=yellow!40]
    \tikzstyle{process} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
    \tikzstyle{arrow} = [thick,->,>=stealth]
    
    % \tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
    % \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
    
    \node[data]    (stim) {stimulus};
    \node[process, right of=stim, xshift=0.8cm] (llm) {LLM};
    \node[prompt,    above of=llm]  (prom)  {generic prompt};
    \node[data,    right of=llm, xshift=0.8cm]  (ta)  {title + abstract};
    \node[process, right of=ta, xshift=0.8cm]   (asr) {ASReview};
    \node[data,    above of=asr]  (syn)  {SYNERGY dataset};
    \node[data,    right of=asr, xshift=0.8cm]  (rel) {relevant papers};
    
    \draw[arrow] (stim) -- (llm);
    \draw[arrow] (prom) -- (llm);
    \draw[arrow] (llm)  -- (ta);
    \draw[arrow] (ta)   -- (asr);
    \draw[arrow] (syn)   -- (asr);
    \draw[arrow] (asr)  -- (rel);
    
    \end{tikzpicture}
    \caption{Simulation pipeline (for more detailed schematic of workings within ASReview see \parencite{asreviewvs2jonathan})}
  \label{fig:sim-pipeline}
\end{figure}

\subsection{Conditions}

This section describes how the three study conditions were operationalised. For each run, the ALM parameters are set to the recommended U4 configuration, except for the no initialisation condition switches between two ALMs (see below). The classifier is then seeded with a default seed plus the run number. 

See exact code: \href{https://github.com/timovanommeren/thesis_timo/blob/main/simulations/simulation.py}{here}

\subsubsection{LLM initialization}

In the LLM initialisation condition, the ALM's classifier is provided with a set of examples comprising at least one relevant and one irrelevant paper before screening is simulated. These examples are generated based on the inclusion and exclusion criteria of the given systematic review publication. See figure \ref{fig:sim-pipeline} for a schematic of the simulation pipeline. 

Between simulation runs the exact number of abstracts generated as well as their specific contents is varied. More specifically, we aimed to investigate the effect of the following variables on starting performance: 
\begin{enumerate}[label=\arabic*)]
\item degree of information provided to the LLM about the systematic review in question \textbf{(3 levels)}, 
\item number of abstracts generated per simulation run \textbf{(4 levels)},
\item length of abstracts generated per simulation run \textbf{(4 levels)},
\item typicality of abstracts generated per simulation run \textbf{(5 levels)},
\item the use of jargon in generated per simulation run \textbf{(5 levels)}.
\end{enumerate}

More specifically, the LLM was provided with either: (a) the title of the published systematic review, (b) the inclusion and exclusion criteria of the systematic review, or (c) the abstract of the published systematic review. Number of abstracts was varied between 1-2-5-10 abstracts per run. The length was varied between 100-200-500-1000 words per run. Whether the abstract should be a typical example of a paper in this review or rather an edge case was varied if 5 or 10 abstracts were generated. In this case the ratio between typical and atypical examples was varied between 00-20-40-60-80-100 \% typical. Finally, the use of jargon was varied similarly to typicality by generating abstracts which were simply pure lists of jargon and combining these with regularly written abstract in  the ratio 00-20-40-60-80-100 given that at least 5 or 10 abstracts were generated. 

To instruct the LLM, a DSPY module was created which takes the variables described above as input, as well as whether the abstract should be relevant or irrelevant \parencite{khattab2023dspy}. Note that to generate multiple abstracts per run, the module was simply looped over and called multiple times. For the exact code, see: \href{https://github.com/timovanommeren/thesis_timo/blob/main/simulations/prompting.py}{here}

\subsubsection{Random initialization}

In the random initialization condition, one relevant and one irrelevant paper were randomly sampled prior to the start of screening. An ALM could therefore be used starting from the very first paper screened. 

\subsubsection{No initialization}

In the no initialisation condition, no papers were provided prior to the start of screening. Therefore, two separate ALMs were used to simulate the screening process. The first ALM randomly sampled papers until at least one relevant and one irrelevant paper were found. These papers were then used to initialise a second ALM, which was used to screen the remaining papers.

\subsection{Outcome variable}

\subsubsection{Starting performance}

Starting performance will be assessed based on the number relevant papers in the first 100 papers screened. This figure is derived from research showing that approximately 100 papers can be screened in an hour \parencite{nussbaumer2021resource}. For each simulation, we count the number of relevant records found with a \textit{Time to Discovery} below 100 for each simulation \parencite{ferdinands2023performance}.

\subsection{Data Generation and Analysis}

\subsubsection{Padding}

It is important to note that the simulation may stop prematurely if all the relevant records are found before the desired stopping rule is reached (e.g. screening 100 records). In order to accurately emulate a researcher who is unaware that all the relevant records have been found and who therefore continues to screen until the stopping rule is reached, the simulation results were appended with rows containing the label 'zero' (i.e. irrelevant records). The number of these rows is determined by taking the number of records that should have been screened (the stopping rule) minus the number of records that were screened (in the random initialisation condition the number of records that were provided prior to screening is also subtracted). This is referred to as padding and ensures that the final simulation results are accurate.

\subsubsection{Recall plots}

A common way of visualizing the simulation results of retrieval tasks in general, and systematic reviews in particular, are recall curves. These curves shows the number of relevant records retrieved at a given number of papers screened. At the end of each simulation run, a recall plot is created containing three curves: one for each condition. Furthermore, at the end of the entire simulation study, an aggregate recall plot is created per dataset, containing the average recall curve of each condition, as well as it's standard error between runs. 

\subsubsection{Exported results}

Each simulation run is stored in a separate CSV file. Every row represents a screened paper and contains the following information:

\begin{enumerate}[label=\arabic*), leftmargin=*, nosep]
  \item The paper's record ID,
  \item The assigned label (i.e., relevant or irrelevant),
  \item The classifier, querier, balancer and feature extractor used,
  \item The size of the training set,
  \item A time tag.
\end{enumerate} 

Furthermore, the following naming convention is used for the CSV files: \newline condition\_run\_run\_IVs\_n\_abstracts\_length\_abstracts\_typicality\_degree\_jargon\_llm\_temperature.csv. The same naming convention is used for the recall plots of each simulation run and for the generated abstracts in the LLM initialisation condition.

Finally, at the end of each run, the current values of the input parameters, the outcome variables and other relevant metadata are appended to a long format master dataframe for analysis. The columns of this dataframe are as follows:

\begin{enumerate}[label=\arabic*), leftmargin=*, nosep]
    \item the name of the outcome variable
    \item the value of the outcome variable
    \item name of the simulated dataset,
    \item the condition,
    \item the values of the independent variables:
        \begin{enumerate}
        \item number of abstracts
        \item length of the abstracts
        \item typicality of the abstracts
        \item degree of jargon in the abstracts
        \item temperature settings of the llm (i.e., diversity)
        \end{enumerate}
    \item timestamp
    \item the seed value
    \item the run number
\end{enumerate}

This yields a data-frame containing one observation for each combination of dataset (n=26), condition (n=3), independent variables and their levels (n=$3 \times 4 \times 4 \times 5 \times 5 = 1200$), and run (n=1), thus with $26 \times 3 \times 1200 \times 1 = 93600$ rows, and the 12 columns enumerated above. 


\subsubsection{Analysis}

The majority of the variance in the number of relevant records found in the first 100 screened is expected to be explained by differences between datasets. We would therefore like to seperate within-dataset variance from between-dataset variance. Ideal for this purpose are  linear mixed-effects models (LMEMs) which allow us to model the effect of condition on starting performance while accounting for between-dataset variance by including dataset as a random effect. To confirm that most of the variance is indeed between datasets, we will visualize the data using boxplots and fit a null model with only dataset as a random effect to calculate the intraclass correlation coefficient (ICC). A bottom-up modelling approach will then be used for multilevel analysis (for approach see chapter 4 of the book by \cite{hox2017multilevel}) and will be conducted in R using the \textit{lme4} package  \parencite{bates2015fitting}. %(version 1.1.36)

We are dealing with structual missing data because not all independent variable levels are applicable to all runs (i.e., the independent variables only have an effect in the LLM initialisation condition). A common approach to deal with this kind of missing data is to use multiple imputation. However, since these variables only have causal meaning in the LLM initialisation condition, imputation does not make conceptual sense. Similarly, model parameter estimation using full maximum likelihood estimation or bayesian estimation would not make conceptual sense (since the MAR assumption would be violated). Another solution would be to simply parse the analysis into two parts: (1) investigate the effect of condition on starting performance, and (2) investigate the effect of the independent variables in the LLM initialisation condition on starting performance. This however would decrease the power of the analysis. Finally, we may reflect the structual missingness in the model by including interaction effects between condition and the independent variables. This way, the independent variables will only have an effect in the LLM initialisation condition. (Note that structual missingness is different from planned missingness.)

One way of dealing with missing data is Dummy Variable Adjustment (DVA). This approach has largely fallen out of favour due to an analysis by \parencite{jones1996indicator} which showed that even when data are missing completely at random (MCAR), DVA produces biased estimates. However, this result is only true if the missing values could plausibly have a value. In our case, the missing values are structurally missing and have no meaning outside of the LLM initialisation condition. Therefore, DVA may be a valid approach to deal with the structural missingness in our data. Furthermore, recent work by \parencite{dziak2017two} show that DVA is algebraically equivalent to including interaction effects between the variable with missing data and the indicator variable for missingness, and imputating the missing data with zero (Note that the model is invarient to the actual imputed value. However, the interpretation of the coeffcients of the other two conditions should be relative to the imputed value. Therefore, mean or zero imputation is recommended.)



\vspace{0.5cm}

Question: how to validate exploratory bottom-up approach to modelling. The book mentions either cross-validation or Benferroni correction.

% then be used to investigate the effect of condition on starting performance. First, a model with only condition as a fixed effect and dataset as a random effect will be fitted. Next, the independent variables in the LLM initialisation condition will be added as fixed effects to investigate whether they explain additional variance in starting performance. Finally, interaction effects between these independent variables and condition will be investigated. Model fit will be assessed using likelihood ratio tests, AIC and BIC. Residuals will be checked for normality and homoscedasticity.



\section{Results (note: preliminary!)}

\subsection{Descriptives}

As expected, most of the variance in starting performance seems to be explained by differences between datasets. Figure \ref{fig:all-datasets} shows the number of relevant records found within the first 100 screened per dataset. There is considerable variation between datasets, with some datasets yielding less than 10 relevant records found across conditions, while other datasets yield more than 50 relevant records across conditions.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{results/td_barchart.png} % adjust path/width
  \caption{The number of relevant records found per dataset}
  \label{fig:all-datasets}
\end{figure}

\subsection{Main results}

\input{results/results_paragraph.tex}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{aggregate_recall_plot_walker.png}
  \includegraphics[width=0.45\textwidth]{aggregate_recall_plot_moran.png}
  \caption{The number of relevant records found per dataset}
  \label{fig:walker-2018}
\end{figure}


\input{results/results_table.tex}

\section{Conclusion} 




\section{Discussion}



\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.2\textwidth]{vector_space/LLM_vector_space.png}
  \includegraphics[width=0.2\textwidth]{vector_space/random_vector_space.png}
  \includegraphics[width=0.2\textwidth]{vector_space/no_vector_space.png}
  \caption{Ideal starting point for systematic reviews using active learning}
  \label{fig:my-image}
\end{figure}


\subsection{Limitations}

\begin{enumerate}
    \item Key limitation simulation study: data leakage (i.e., LLMs have been trained on synergy datasets). The ecological validity of the results are therefore somewhat limited.
    \begin{enumerate}
        \item There are two obvious solutions to the problem of data leakage: (1) apply the use of LLm-initialization on a new systematic review, and (2) use an older LLM from hugging face for example. 
    \end{enumerate}
    \item Another possible limitation: no switching of active leaning cycles (could fix contamination of synthetic data issue). 
\end{enumerate}

Clusters should not be a problem because we only focus on starting performance (i.e., first 100 screened). Future work may consider the contamination hypothesis in more detail. 

%TC:ignore

\appendix

\section{SYNERGY metadata}

\begin{table}[ht]
\centering
\caption{Datasets overview}
\begin{tabularx}{\textwidth}{@{}r l X r r r@{}}
\toprule
\textbf{Nr} & \textbf{Dataset} & \textbf{Topic(s)} & \textbf{Records} & \textbf{Included} & \textbf{\%} \\
\midrule
1  & Appenzeller-Herzog\_2019 & Medicine & 2873  & 26  & 0.9 \\
2  & Bos\_2018                 & Medicine & 4878  & 10  & 0.2 \\
3  & Brouwer\_2019             & Psychology, Medicine & 38114 & 62  & 0.2 \\
4  & Chou\_2003                & Medicine & 1908  & 15  & 0.8 \\
5  & Chou\_2004                & Medicine & 1630  & 9   & 0.6 \\
6  & Donners\_2021             & Medicine & 258   & 15  & 5.8 \\
7  & Hall\_2012                & Computer science & 8793  & 104 & 1.2 \\
8  & Jeyaraman\_2020           & Medicine & 1175  & 96  & 8.2 \\
9  & Leenaars\_2019            & Psychology, Chemistry, Medicine & 5812  & 17  & 0.3 \\
10 & Leenaars\_2020            & Medicine & 7216  & 583 & 8.1 \\
11 & Meijboom\_2021            & Medicine & 882   & 37  & 4.2 \\
12 & Menon\_2022               & Medicine & 975   & 74  & 7.6 \\
13 & Moran\_2021               & Biology, Medicine & 5214  & 111 & 2.1 \\
14 & Muthu\_2021               & Medicine & 2719  & 336 & 12.4 \\
15 & Nelson\_2002              & Medicine & 366   & 80  & 21.9 \\
16 & Oud\_2018                 & Psychology, Medicine & 952   & 20  & 2.1 \\
17 & Radjenovic\_2013          & Computer science & 5935  & 48  & 0.8 \\
18 & Sep\_2021                 & Psychology & 271   & 40  & 14.8 \\
19 & Smid\_2020                & Computer science, Mathematics & 2627  & 27  & 1.0 \\
20 & van\_de\_Schoot\_2018     & Psychology, Medicine & 4544  & 38  & 0.8 \\
21 & van\_der\_Valk\_2021      & Medicine, Psychology & 725   & 89  & 12.3 \\
22 & van\_der\_Waal\_2022      & Medicine & 1970  & 33  & 1.7 \\
23 & van\_Dis\_2020            & Psychology, Medicine & 9128  & 72  & 0.8 \\
24 & Walker\_2018              & Biology, Medicine & 48375 & 762 & 1.6 \\
25 & Wassenaar\_2017           & Medicine, Biology, Chemistry & 7668  & 111 & 1.4 \\
26 & Wolters\_2018             & Medicine & 4280  & 19  & 0.4 \\
\bottomrule
\end{tabularx}
\end{table}
\label{tab:SYNERGY}

% References
\printbibliography
%TC:endignore

\end{document}
